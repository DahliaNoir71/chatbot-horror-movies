"""Prometheus metrics definitions for AI services monitoring (C8).

Defines metric objects for LLM, intent classifier, and embedding services.
The middleware exposing /metrics is implemented in C11 (E3).
"""

from prometheus_client import Counter, Gauge, Histogram, Info

# =============================================================================
# LLM METRICS
# =============================================================================

LLM_REQUEST_DURATION = Histogram(
    "horrorbot_llm_request_duration_seconds",
    "LLM inference latency in seconds",
    buckets=[0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0],
)

LLM_TOKENS_GENERATED = Counter(
    "horrorbot_llm_tokens_generated_total",
    "Total tokens generated by LLM",
)

LLM_PROMPT_TOKENS = Counter(
    "horrorbot_llm_prompt_tokens_total",
    "Total prompt tokens processed by LLM",
)

LLM_TOKENS_PER_SECOND = Gauge(
    "horrorbot_llm_tokens_per_second",
    "Current LLM generation throughput (tokens/s)",
)

LLM_REQUESTS_TOTAL = Counter(
    "horrorbot_llm_requests_total",
    "Total LLM inference requests",
    ["status"],  # success, error, timeout
)

# =============================================================================
# INTENT CLASSIFIER METRICS
# =============================================================================

CLASSIFIER_REQUEST_DURATION = Histogram(
    "horrorbot_classifier_request_duration_seconds",
    "Intent classifier inference latency in seconds",
    buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5],
)

CLASSIFIER_REQUESTS_TOTAL = Counter(
    "horrorbot_classifier_requests_total",
    "Total intent classification requests",
    ["intent"],  # horror_recommendation, film_details, horror_discussion, etc.
)

CLASSIFIER_CONFIDENCE = Histogram(
    "horrorbot_classifier_confidence",
    "Intent classifier confidence score distribution",
    buckets=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
)

# =============================================================================
# EMBEDDING METRICS
# =============================================================================

EMBEDDING_REQUEST_DURATION = Histogram(
    "horrorbot_embedding_request_duration_seconds",
    "Embedding encoding latency in seconds",
    buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0],
)

# =============================================================================
# MEMORY METRICS
# =============================================================================

MODEL_MEMORY_BYTES = Gauge(
    "horrorbot_model_memory_bytes",
    "Memory used by loaded AI models in bytes",
    ["model"],  # llm, classifier, embedding
)

# =============================================================================
# MODEL INFO
# =============================================================================

MODEL_INFO = Info(
    "horrorbot_model",
    "Information about loaded AI models",
)
