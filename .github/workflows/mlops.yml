# .github/workflows/mlops.yml
# ===================================================
# Pipeline MLOps â€” Evaluation des modeles IA HorrorBot
# ===================================================
# Evalue la classification d'intents, la qualite des embeddings et la
# qualite des reponses RAG. Genere un rapport consolide et cree une PR
# de livraison si tous les seuils sont respectes.
#
# Justification "entrainement" : les modeles HorrorBot sont pre-entraines
# (DeBERTa-v3 zero-shot, all-MiniLM-L6-v2, Qwen2.5-7B GGUF). La
# generation d'embeddings constitue l'equivalent fonctionnel de
# l'entrainement. Voir G18 dans le rapport E3.

name: MLOps Pipeline

# ===================================================
# Declencheurs
# ===================================================
on:
  # Push sur les fichiers de donnees ou modeles
  push:
    branches: [main, develop]
    paths:
      - 'tests/fixtures/**'
      - 'src/services/embedding/**'
      - 'src/services/intent/**'
      - 'src/services/rag/**'
      - 'tests/model/**'
      - 'scripts/generate_sample_embeddings.py'
      - 'scripts/mlops_report.py'
      - '.github/workflows/mlops.yml'

  # Execution hebdomadaire (dimanche 02:00 UTC)
  schedule:
    - cron: '0 2 * * 0'

  # Declenchement manuel avec option de skip du PR
  workflow_dispatch:
    inputs:
      skip_release_pr:
        description: 'Ne pas creer de PR de livraison'
        type: boolean
        default: false

# ===================================================
# Variables d'environnement globales
# ===================================================
env:
  PYTHON_VERSION: "3.12"
  PYTHONUNBUFFERED: "1"
  HF_HOME: ~/.cache/huggingface
  TRANSFORMERS_CACHE: ~/.cache/huggingface/hub

# ===================================================
# Permissions pour la creation de PR
# ===================================================
permissions:
  contents: write
  pull-requests: write

# ===================================================
# JOBS
# ===================================================
jobs:

  # ============================================
  # JOB 1 : Validation des donnees
  # ============================================
  # Execute test_data_validation.py (21 tests)
  # Valide les structures de donnees et l'integrite des fixtures
  validate-data:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      # Necessite --group ml a cause des imports transitifs
      # (embedding_service.py -> sentence_transformers)
      - name: Install dependencies
        run: uv sync --locked --group dev --group ml

      - name: Cache HuggingFace models
        uses: actions/cache@v4
        with:
          path: ~/.cache/huggingface/hub
          key: huggingface-${{ hashFiles('.env.test') }}
          restore-keys: |
            huggingface-

      - name: Run data validation tests
        env:
          ENVIRONMENT: test
        run: |
          mkdir -p reports/validate-data
          uv run pytest tests/model/test_data_validation.py \
            -m "model" \
            -v --tb=short \
            --no-header \
            -p no:cacheprovider \
            --override-ini="addopts=" \
            --junitxml=reports/validate-data/junit.xml \
            2>&1 | tee reports/validate-data/output.log

      - name: Upload validation results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: validate-data-results
          path: reports/validate-data/
          retention-days: 30

  # ============================================
  # JOB 2 : Accuracy du classificateur d'intents
  # ============================================
  # Execute test_intent_accuracy.py avec le vrai modele DeBERTa-v3
  # Genere la matrice de confusion en JSON
  test-intent-classifier:
    runs-on: ubuntu-latest
    needs: [validate-data]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Install dependencies (with ML)
        run: uv sync --locked --group dev --group ml

      - name: Cache HuggingFace models
        uses: actions/cache@v4
        with:
          path: ~/.cache/huggingface/hub
          key: huggingface-${{ hashFiles('.env.test') }}
          restore-keys: |
            huggingface-

      - name: Run intent accuracy tests
        env:
          ENVIRONMENT: test
          MLOPS_ARTIFACT_DIR: reports/intent-classifier
        run: |
          mkdir -p reports/intent-classifier
          uv run pytest tests/model/test_intent_accuracy.py \
            -m "model" \
            -v --tb=short -s \
            --no-header \
            -p no:cacheprovider \
            --override-ini="addopts=" \
            --junitxml=reports/intent-classifier/junit.xml \
            2>&1 | tee reports/intent-classifier/output.log

      - name: Upload intent classifier results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: intent-classifier-results
          path: reports/intent-classifier/
          retention-days: 30

  # ============================================
  # JOB 3 : Generation d'embeddings + qualite
  # ============================================
  # Execute generate_sample_embeddings.py (equivalent entrainement)
  # puis test_embedding_quality.py pour validation
  generate-embeddings:
    runs-on: ubuntu-latest
    needs: [validate-data]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Install dependencies (with ML)
        run: uv sync --locked --group dev --group ml

      - name: Cache HuggingFace models
        uses: actions/cache@v4
        with:
          path: ~/.cache/huggingface/hub
          key: huggingface-${{ hashFiles('.env.test') }}
          restore-keys: |
            huggingface-

      - name: Generate sample embeddings (equivalent entrainement)
        env:
          ENVIRONMENT: test
        run: |
          mkdir -p reports/embeddings
          uv run python scripts/generate_sample_embeddings.py \
            --output-dir reports/embeddings \
            --fixtures-dir tests/fixtures

      - name: Run embedding quality tests
        env:
          ENVIRONMENT: test
        run: |
          uv run pytest tests/model/test_embedding_quality.py \
            -m "model" \
            -v --tb=short -s \
            --no-header \
            -p no:cacheprovider \
            --override-ini="addopts=" \
            --junitxml=reports/embeddings/junit.xml \
            2>&1 | tee reports/embeddings/output.log

      - name: Upload embedding results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: embeddings-results
          path: reports/embeddings/
          retention-days: 30

  # ============================================
  # JOB 4 : Evaluation qualite RAG
  # ============================================
  # Execute test_response_quality.py (validation templates + prompts)
  evaluate-rag:
    runs-on: ubuntu-latest
    needs: [validate-data]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      # Necessite --group ml a cause des imports transitifs
      - name: Install dependencies
        run: uv sync --locked --group dev --group ml

      - name: Cache HuggingFace models
        uses: actions/cache@v4
        with:
          path: ~/.cache/huggingface/hub
          key: huggingface-${{ hashFiles('.env.test') }}
          restore-keys: |
            huggingface-

      - name: Run response quality tests
        env:
          ENVIRONMENT: test
        run: |
          mkdir -p reports/rag-evaluation
          uv run pytest tests/model/test_response_quality.py \
            -m "model" \
            -v --tb=short \
            --no-header \
            -p no:cacheprovider \
            --override-ini="addopts=" \
            --junitxml=reports/rag-evaluation/junit.xml \
            2>&1 | tee reports/rag-evaluation/output.log

      - name: Upload RAG evaluation results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: rag-evaluation-results
          path: reports/rag-evaluation/
          retention-days: 30

  # ============================================
  # JOB 5 : Generation du rapport consolide
  # ============================================
  # Telecharge tous les artifacts et genere un rapport markdown
  generate-report:
    runs-on: ubuntu-latest
    needs: [test-intent-classifier, generate-embeddings, evaluate-rag]
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v7
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Install dependencies (dev only)
        run: uv sync --locked --group dev

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: reports/

      - name: List downloaded artifacts
        run: find reports/ -type f | head -50

      - name: Generate MLOps report
        env:
          GITHUB_RUN_ID: ${{ github.run_id }}
          GITHUB_SHA: ${{ github.sha }}
          GITHUB_REF: ${{ github.ref }}
          GITHUB_REPOSITORY: ${{ github.repository }}
        run: |
          uv run python scripts/mlops_report.py \
            --artifacts-dir reports/ \
            --output reports/mlops-report.md

      - name: Upload consolidated report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: mlops-report
          path: reports/mlops-report.md
          retention-days: 90

  # ============================================
  # JOB 6 : Creation de la PR de livraison
  # ============================================
  # Cree une PR avec le rapport d'evaluation si tous les seuils passent
  create-release-pr:
    runs-on: ubuntu-latest
    needs: [generate-report, test-intent-classifier, generate-embeddings, evaluate-rag]
    if: >
      needs.generate-report.result == 'success' &&
      needs.test-intent-classifier.result == 'success' &&
      needs.generate-embeddings.result == 'success' &&
      needs.evaluate-rag.result == 'success' &&
      github.event.inputs.skip_release_pr != 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download MLOps report
        uses: actions/download-artifact@v4
        with:
          name: mlops-report
          path: reports/

      - name: Configure git
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

      - name: Create release branch and PR
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          BRANCH="mlops/report-${TIMESTAMP}"

          git checkout -b "${BRANCH}"

          mkdir -p docs/mlops-reports
          cp reports/mlops-report.md "docs/mlops-reports/report-${TIMESTAMP}.md"
          cp reports/mlops-report.md docs/mlops-reports/LATEST.md

          git add docs/mlops-reports/
          git commit -m "docs(mlops): add evaluation report ${TIMESTAMP}"
          git push origin "${BRANCH}"

          gh pr create \
            --title "MLOps Report ${TIMESTAMP}" \
            --body "$(cat <<'EOF'
          ## Rapport d'evaluation MLOps

          Evaluation automatique des modeles IA terminee avec succes.
          Tous les seuils de qualite sont respectes.

          ### Resume des jobs
          - **validate-data** : PASSED
          - **test-intent-classifier** : PASSED
          - **generate-embeddings** : PASSED
          - **evaluate-rag** : PASSED

          Le rapport complet est dans `docs/mlops-reports/LATEST.md`.

          ---
          *Genere par le pipeline MLOps (run #${{ github.run_id }})*
          EOF
          )" \
            --base main \
            --head "${BRANCH}"
